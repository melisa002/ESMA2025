{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67bd6a10-7ddc-456f-99fe-46668210c23c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ESMA and the department\n",
    "- Organigramme\n",
    "- What we do: examples of analysis\n",
    "- What we do: Medallion architecture\n",
    "- What we do: statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96276106-1515-4387-be99-52114ae6e284",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Organigramme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0753931e-46b0-45d4-9628-6501f0b4eb21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Analysis examples:\n",
    "Collaborate with other departments on the data part of reports such as the Trends, Risks and Vulnerabilities (TRV):\n",
    "https://www.esma.europa.eu/document/trends-risks-and-vulnerabilities-trv-report-no-2-2024\n",
    "\n",
    "Which contains regular charts and ad-hoc analysis such as the gas derivatives:\n",
    "https://www.esma.europa.eu/document/trv-article-eu-natural-gas-derivatives-markets-risks-and-trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3171a833-2807-4dae-8e31-837fa8c02dd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Medallion architecture:\n",
    "- https://www.databricks.com/glossary/medallion-architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98335000-9df5-4b26-9a65-4e3a35c8af0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Statistics\n",
    "- Example from the ECB data portal: https://data.ecb.europa.eu/data/datasets/BSI/BSI.M.U2.Y.U.A20T.A.I.U2.2250.Z01.A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b38dbeab-3e5a-41f3-b2b9-608c222b96d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Note:** The outlier methods we will develop would fit around the two/three topics above: as alerts when producing statistics and (if time allows it) to explore the most granular golden tables to produce articles or reports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dadbefbd-098d-4f62-8f59-a10c034df43f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Intro to Databricks\n",
    "- Scheduled tasks\n",
    "- Pyspark and big data\n",
    "- Outlier method example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea630395-3b2b-4e11-872f-2110d981a0f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Scheduled tasks: \n",
    "Regular tasks such as maintaining the medallion architecture and the computation of statistics can (if the data behaves decently) be automated.\n",
    "\n",
    "We need to keep that in mind for the outlier methods: so that they can be used manually (I want to explore this granular dataset) but also scheduled (after computing X, I run this and get alerts)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "572f6878-c055-4331-9fe2-29bdca374d69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Pyspark\n",
    "Prefered method to work (as opposed to pandas dataframes, for example). Several reasons:\n",
    "- Paralellization: Makes use of big data features such as having several clusters per worker. \n",
    "- Memory use: does not store all the data in a single 'computer' within the cluster.\n",
    "- Lazy evaluation: Actions are only triggered when necessary. \n",
    "\n",
    "Documentation: \n",
    "  - https://spark.apache.org/docs/latest/api/python/index.html\n",
    "  - https://medium.com/@john_tringham/spark-concepts-simplified-lazy-evaluation-d398891e0568\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "309db315-f2f7-4464-840f-335ec7826d16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>value</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1992</td>\n",
       "      <td>0.009401</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024</td>\n",
       "      <td>0.395331</td>\n",
       "      <td>Nepal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1928</td>\n",
       "      <td>0.065743</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1950</td>\n",
       "      <td>0.241188</td>\n",
       "      <td>Nepal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008</td>\n",
       "      <td>0.084063</td>\n",
       "      <td>Nepal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year     value country\n",
       "0  1992  0.009401   India\n",
       "1  2024  0.395331   Nepal\n",
       "2  1928  0.065743   India\n",
       "3  1950  0.241188   Nepal\n",
       "4  2008  0.084063   Nepal"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some examples\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# pyspark already loaded when opening a notebook, but these are the useful functions we'll use most of the time\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "# first let's make a fake pandas dataframe and turn it into spark\n",
    "\n",
    "# Define the number of rows\n",
    "num_rows = 100000\n",
    "\n",
    "# Generate random years\n",
    "years = np.random.choice(range(1900, 2026), num_rows)\n",
    "\n",
    "# Generate random countries from the list\n",
    "countries = np.random.choice(['Japan', 'Nepal', 'India'], num_rows)\n",
    "\n",
    "# Generate random values, multiplied by another random number to add noise\n",
    "values = np.random.rand(num_rows) * np.random.rand(num_rows)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'year': years,\n",
    "    'value': values,\n",
    "    'country': countries\n",
    "})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4890ff53-4327-4821-8c6a-095b5ace5840",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/09 13:06:36 WARN Utils: Your hostname, Mels-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.99.64 instead (on interface en0)\n",
      "25/04/09 13:06:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/09 13:06:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/04/09 13:06:36 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"SparkSQL.com\").getOrCreate()\n",
    "dfs = spark.createDataFrame(df)\n",
    "\n",
    "# see the type of object: pyspark connect dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01c6d607-ef8c-41c2-8d8e-5ddba442fe9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[year: bigint, value: double, country: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'running' it just shows the schema\n",
    "dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebe86a07-132d-4754-a90e-8697469a80b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Scan ExistingRDD[year#0L,value#1,country#2]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# explaining it shows what it is: essentially an SQL query\n",
    "# in this case reading a 'local' table (the pandas dataframe just created, stored in the driver)\n",
    "dfs.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6555a572-1dc9-47e1-a798-efc7fac5defb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[year: bigint, value: double, country: string, value_thousands: double, country_upper: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# altering the dataframe does not trigger any action (lazy evaluation)\n",
    "\n",
    "# let's add a couple of columns\n",
    "dfs = dfs.withColumn('value_thousands', f.col('value')/1000)\n",
    "\n",
    "dfs = dfs.withColumn('country_upper', f.upper('country'))\n",
    "\n",
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53aab93f-fc36-49f2-91e4-cb109d368790",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/09 13:07:08 WARN TaskSetManager: Stage 0 contains a task of very large size (2159 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-------+--------------------+-------------+\n",
      "|year|               value|country|     value_thousands|country_upper|\n",
      "+----+--------------------+-------+--------------------+-------------+\n",
      "|1922|  0.3626450669113175|  Nepal|3.626450669113175E-4|        NEPAL|\n",
      "|1972|  0.2301452471932439|  Japan|2.301452471932439E-4|        JAPAN|\n",
      "|1939|  0.2241952003370295|  India|2.241952003370295E-4|        INDIA|\n",
      "|2021|  0.6892271723973562|  India|6.892271723973562E-4|        INDIA|\n",
      "|1929| 0.27128380141492636|  Nepal|2.712838014149264E-4|        NEPAL|\n",
      "|1983| 0.19938612442183518|  Nepal|1.993861244218351...|        NEPAL|\n",
      "|1942|  0.2033969439462529|  Nepal|2.033969439462529E-4|        NEPAL|\n",
      "|1961|  0.3873328151303006|  Japan|3.873328151303006E-4|        JAPAN|\n",
      "|1962|  0.0648391518169159|  India| 6.48391518169159E-5|        INDIA|\n",
      "|1915| 0.15921067049825194|  Japan|1.592106704982519...|        JAPAN|\n",
      "|1921| 0.37760778453827665|  Nepal|3.776077845382766...|        NEPAL|\n",
      "|1926| 0.26686670741038626|  Nepal|2.668667074103863E-4|        NEPAL|\n",
      "|2008|  0.4096922945395366|  Japan|4.096922945395366E-4|        JAPAN|\n",
      "|2022| 0.44819309301655597|  India| 4.48193093016556E-4|        INDIA|\n",
      "|1973| 0.03499165507420945|  India|3.499165507420945E-5|        INDIA|\n",
      "|1990| 0.04251869904350551|  Japan|4.251869904350551E-5|        JAPAN|\n",
      "|1979| 0.21960794111570414|  India|2.196079411157041...|        INDIA|\n",
      "|1942|0.007422047210128504|  India|7.422047210128504E-6|        INDIA|\n",
      "|1936| 0.13408405273805077|  Japan|1.340840527380507...|        JAPAN|\n",
      "|1946| 0.11318586842968689|  India|1.131858684296869E-4|        INDIA|\n",
      "+----+--------------------+-------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# the computations above are only triggered whenever they are needed\n",
    "# for example, with a command such as display\n",
    "dfs.sample(0.001).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b48f92d3-1c01-45fa-ad4c-4bb773f26642",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Ideally, all the code we develop would be in Pyspark, which can also run on a local computer without being in big data.\n",
    "- If that is problematic, we can also write it in Polars and then I convert it to pyspark internally. https://pola.rs/\n",
    "- For example, polars with_columns works almost identically as pyspark withColumn above\n",
    "https://docs.pola.rs/api/python/stable/reference/dataframe/api/polars.DataFrame.with_columns.html\n",
    "\n",
    "- If, in certain sections, we need to use pandas (for example, for a fancier outlier model), we can optionally convert to pandas and then back to spark. This might be dangerous with big datasets but it can be an option for the users.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c54aebb-b647-4879-9743-bcd03ff14f7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Outlier method example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52462dbf-a53d-4a5d-bebd-5aeaa91db205",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's run a pyspark function on it, to flag outliers\n",
    "from functools import reduce #for eqnullsafe joins, but we can simplify this\n",
    "\n",
    "def add_outlier_thresholds(data, numbercol, groupbycols: list[str]=[], showstats=False, use_logs: bool=False):\n",
    "    \"\"\"\n",
    "    Identifies outliers in the specified column based on whether they exceed the median by\n",
    "    over 3 or 4 standard deviations (of the logarithm of the absolute value).\n",
    "\n",
    "    Paramerers\n",
    "    ----------\n",
    "    data: pyspark.sql.DataFrame\n",
    "        The dataframe containing the data for which to compute the thresholds.\n",
    "    numbercol: str\n",
    "        Name of the column for which to compute the thresholds.\n",
    "    groupbycols: str or list, Optional\n",
    "        Name(s) of column(s) based on which to group the data to generate different\n",
    "        thresholds for different groups.\n",
    "    stats: bool, default False\n",
    "        If True, prints a count of the number of outliers identified.\n",
    "    use_logs: bool, default False\n",
    "        If true, we compare the logarithm of numbercol with the thresholds.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pyspark.sql.DataFrame:\n",
    "        The original dataframe with two additional columns, one for the 3sd metric and\n",
    "        another for 4sd. The columns are populated with True if the record is identified\n",
    "        as an outlier.\n",
    "    \"\"\"\n",
    "\n",
    "    # new column names\n",
    "    col3sd = numbercol + '_3sd'\n",
    "    col4sd = numbercol + '_4sd'\n",
    "\n",
    "    # group also by date to prevent obfuscating metrics across dates\n",
    "    #groupbycols += ['AS_OF_DATE'] # unused in this example\n",
    "\n",
    "    # compute mean and standard deviation\n",
    "    stats = (data\n",
    "             .groupBy(groupbycols)\n",
    "             .agg(\n",
    "                 f.median(f.col(numbercol).cast('float')).alias('median'),\n",
    "                 f.stddev(f.col(numbercol).cast('float')).alias('stddev')\n",
    "             )\n",
    "             .withColumn('3sd', f.col('median') + f.col('stddev')*3 )\n",
    "             .withColumn('4sd', f.col('median') + f.col('stddev')*4 ))\n",
    "\n",
    "    # join threshold data\n",
    "\n",
    "    if groupbycols:\n",
    "        # Create a hash column based on the groupby columns in both DataFrames.\n",
    "        stats_hashed = stats.select(\"*\", f.hash(*groupbycols).alias(\"hash\")) \\\n",
    "                            .dropDuplicates([\"hash\"])\n",
    "        data_hashed = data.select(\"*\", f.hash(*groupbycols).alias(\"hash\"))\n",
    "\n",
    "        # Perform a left join on the hash column.\n",
    "        data = data_hashed.alias(\"data\") \\\n",
    "                    .join(stats_hashed.alias(\"stats\"), on=\"hash\", how=\"left\") \\\n",
    "                    .drop(\"hash\")\n",
    "\n",
    "    else: # otherwise make dummy column to join the single-row stats data\n",
    "        data = (data\n",
    "                .withColumn('dummykey', f.lit(1))\n",
    "                .join(stats.withColumn('dummykey', f.lit(1)), 'dummykey', 'left')\n",
    "                .drop('dummykey'))\n",
    "\n",
    "    # create column for values exceeding 3 standard deviations and drop stats columns\n",
    "    if use_logs:\n",
    "       data = (data\n",
    "            .withColumn(col3sd, f.when(f.log(f.abs(f.col(numbercol))) > f.col('3sd'),\n",
    "                       f.lit(True)).otherwise(f.lit(False)) )\n",
    "            .withColumn(col4sd, f.when(f.log(f.abs(f.col(numbercol))) > f.col('4sd'),\n",
    "                       f.lit(True)).otherwise(f.lit(False))   )\n",
    "            .drop('median', 'stddev', '3sd', '4sd'))\n",
    "    else:\n",
    "        data = (data\n",
    "            .withColumn(col3sd, f.when(f.col(numbercol) > f.col('3sd'),\n",
    "                       f.lit(True)).otherwise(f.lit(False)) )\n",
    "            .withColumn(col4sd, f.when(f.col(numbercol) > f.col('4sd'),\n",
    "                       f.lit(True)).otherwise(f.lit(False))   )\n",
    "            .drop('median', 'stddev', '3sd', '4sd'))\n",
    "\n",
    "    # print number of outliers detected\n",
    "    if showstats:\n",
    "        print(f\"Number of over-3sd outliers: {data.select(col3sd).filter(f.col(col3sd) == True).count()}\")\n",
    "        print(f\"Number of over-4sd outliers: {data.select(col4sd).filter(f.col(col4sd) == True).count()}\")\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42d1167e-486e-47e8-890e-898227c2f44c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/09 13:18:13 WARN TaskSetManager: Stage 2 contains a task of very large size (2159 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/04/09 13:18:13 WARN TaskSetManager: Stage 3 contains a task of very large size (2159 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of over-3sd outliers: 1213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/09 13:18:14 WARN TaskSetManager: Stage 14 contains a task of very large size (2159 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/04/09 13:18:14 WARN TaskSetManager: Stage 15 contains a task of very large size (2159 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of over-4sd outliers: 0\n"
     ]
    }
   ],
   "source": [
    "df_test = add_outlier_thresholds(dfs, 'value', groupbycols=['country'], showstats=True)\n",
    "\n",
    "#df_test.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "340b330f-e457-458f-bd9a-37f3968f66e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Next steps \n",
    "- Jordi to share notebook\n",
    "- Test with fake data\n",
    "- Test packages (pyspark, polars), default outlier model\n",
    "\n",
    "#Next meeting(s)\n",
    "- prioritization\n",
    "- introduction to prospectus https://www.esma.europa.eu/issuer-disclosure/prospectus\n",
    "and its dataset https://registers.esma.europa.eu/publication/searchRegister?core=esma_registers_priii_securities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cec1508f-900c-401f-b6bd-e5a4167641e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def melisa_outliers(data,\n",
    "                    function = add_outlier_thresholds,\n",
    "                    min_filter,\n",
    "                    min_date):\n",
    "\n",
    "    if function = 'random_forest_regressor':\n",
    "        df = df.toPandas()\n",
    "\n",
    "        # pandas block\n",
    "\n",
    "        df = spark.createDaframe(df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "BSE introduction(1)",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "datasci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
